# Multi-Agent RAG Customer Support System

[ro-anderson](https://github.com/ro-anderson) | [LinkedIn](https://www.linkedin.com/in/ro-anderson/)

![app running](./images/sample.gif)

## What is this project?

This project implements a multi-agent Retrieval-Augmented Generation (RAG) system for customer support. It uses Python, LangChain, and LangGraph to create a conversational AI that can assist with various travel-related queries, including flight bookings, car rentals, hotel reservations, and excursions.


### Multi-Agent RAG System Graph
![app running](./graphs/multi-agent-rag-system-graph.png)

#### Architecture Overview
The system is built using a multi-agent architecture, implemented as a state graph using LangGraph. Here's a brief explanation of the main components:

1. **Primary Assistant**: This is the entry point for user queries. It routes the conversation to specialized assistants based on the user's needs.
2. **Specialized Assistants**:
    - **Flight Booking Assistant**: Handles flight-related queries and bookings.
    - **Car Rental Assistant**: Manages car rental requests.
    - **Hotel Booking Assistant**: Processes hotel reservation queries.
    - **Excursion Assistant**: Handles trip and excursion recommendations.
3. **Tool Nodes**: Each assistant has access to both *```safe```* and *```sensitive```* tools. Safe tools can be used without user confirmation, while sensitive tools require user approval before execution.
4. **Routing Logic**: The system uses *```conditional edges```* to route the conversation between assistants and *```tool nodes```* based on the current state and user input.
5. **User Confirmation**: For sensitive operations, the system pauses and asks for user confirmation before proceeding.
6. **Memory and State Management**: The system maintains conversation state and uses a *```memory checkpointer```* to save progress.

This architecture allows for a flexible and modular approach to handling diverse customer support scenarios, with built-in safety measures for sensitive operations.
## Requirements

- Python 3.12+
- [Poetry](https://python-poetry.org/docs/#installation)
- Docker and Docker Compose
- OpenAI API Key
- LangSmith API Key (optional, for tracing)

## How to run?

### Local Setup

1. Clone the repository and navigate to the project directory.

2. Create a `.env` file from `.dev.env`:
```bash
cp .dev.env .env
```


3. Edit the `.env` file and fill in the required values:

```bash
OPENAI_API_KEY="your_openai_api_key"
LANGCHAIN_API_KEY="your_langsmith_api_key" # Optional
```


4. Install dependencies:

```bash
poetry install
```


5. Generate embeddings:

```bash
poetry run python vectorizer/app/main.py
```


6. Start the Qdrant vector database:

```bash
docker compose up qdrant -d
```


   You can access the Qdrant UI at: http://localhost:6333/dashboard#

7. Run the customer support chat system:

```bash
poetry run python ./customer_support_chat/app/main.py
```

## Project Structure

This project consists of two main services:

1. **Vectorizer**: Generates embeddings for the knowledge base.
2. **Customer Support Chat**: The main conversational AI system.

The Customer Support Chat service depends on the vector database generated by the Vectorizer.

## Data Source and Vector Database

This project uses two main data sources:

1. Travel Database:
A travel database benchmark from LangGraph. This SQLite database contains information about flights, bookings, passengers, and other travel-related data.

![Database Schema](./images/travel_db_schema.png)

Data source: [LangGraph Travel DB Benchmark](https://storage.googleapis.com/benchmarks-artifacts/travel-db)

2. Qdrant:
A vector database used to store and query the embeddings of the travel database.
![Database Schema](./images/qdrant_schema.png)
